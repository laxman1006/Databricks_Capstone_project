{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f6fcd0-4144-4f16-b955-4a401e498c93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "103898f6-089b-4254-80ea-4c8882a5d991",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Old working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8822a9d-0726-4754-8e82-7a59f5513404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions df to fraud flags streaming: True\nFraud Flags DataFrame is streaming: True\ntransactions df for customer segments streaming: True\ncustomers df is streaming: True\nCustomer Segments DataFrame is streaming: True\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_timestamp, concat_ws, sha2, sum as _sum, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# function for checking if DF is streaming\n",
    "def is_streaming(df):\n",
    "    return hasattr(df, 'isStreaming') and df.isStreaming\n",
    "\n",
    "# defining UDF to generate flag_id example 'F001'\n",
    "@udf(StringType())\n",
    "def generate_flag_id(transaction_id):\n",
    "    prefix = \"F\"\n",
    "    suffix = str(transaction_id).zfill(3)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "# defining transformation logic to fraud_flags table\n",
    "def create_fraud_flags(transactions_df):\n",
    "    fraud_flags_df = transactions_df \\\n",
    "        .withColumn(\"flag_type\", when(col(\"amount\") > 50000, \"unusual_amount\")\n",
    "                    .when(col(\"channel\") == \"mobile\", \"velocity_check\")\n",
    "                    .when(col(\"transaction_type\") == \"transfer\", \"pattern_anomaly\")\n",
    "                    .otherwise(lit(None))) \\\n",
    "        .withColumn(\"confidence_score\", when(col(\"flag_type\") == \"unusual_amount\", 0.9)\n",
    "                    .when(col(\"flag_type\") == \"velocity_check\", 0.8)\n",
    "                    .when(col(\"flag_type\") == \"pattern_anomaly\", 0.7)\n",
    "                    .otherwise(lit(0.5))) \\\n",
    "        .withColumn(\"flag_id\", generate_flag_id(col(\"transaction_id\")))\n",
    "\n",
    "    fraud_flags_df = fraud_flags_df \\\n",
    "        .filter(col(\"flag_type\").isNotNull()) \\\n",
    "        .select(\"flag_id\", \"transaction_id\", \"flag_type\", \"confidence_score\", \"timestamp\")\n",
    "\n",
    "    return fraud_flags_df\n",
    "\n",
    "# defining transformation logic to customer_segments table\n",
    "def create_customer_segments(transactions_df, customers_df):\n",
    "    # converting timestamp to TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # adding watermark to handling late arrival data\n",
    "    transactions_df = transactions_df.withWatermark(\"timestamp\", \"1 day\")\n",
    "\n",
    "    # doing transaction and customers df in temporary view\n",
    "    transactions_df.createOrReplaceTempView(\"transactions_view\")\n",
    "    customers_df.createOrReplaceTempView(\"customers_view\")\n",
    "\n",
    "    # sql queries for streaming aggregation\n",
    "    high_value_query = \"\"\"\n",
    "        SELECT customer_id, 'High_Value' AS segment_name, 'Customers with high transaction volume' AS segment_description\n",
    "        FROM (\n",
    "            SELECT customer_id, SUM(amount) AS total_amount\n",
    "            FROM transactions_view\n",
    "            GROUP BY customer_id\n",
    "            HAVING SUM(amount) > 100000\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    new_user_query = \"\"\"\n",
    "        SELECT customer_id, 'New_User' AS segment_name, 'Customers who joined in last 30 days' AS segment_description\n",
    "        FROM customers_view\n",
    "        WHERE join_date > current_date() - INTERVAL 30 DAYS\n",
    "    \"\"\"\n",
    "\n",
    "    high_value_df = spark.sql(high_value_query)\n",
    "    new_user_df = spark.sql(new_user_query)\n",
    "\n",
    "    # combining segments\n",
    "    customer_segments_df = high_value_df.union(new_user_df) \\\n",
    "        .withColumn(\"segment_id\", sha2(concat_ws(\"_\", col(\"customer_id\"), col(\"segment_name\")), 256)) \\\n",
    "        .withColumn(\"last_updated\", current_timestamp())\n",
    "\n",
    "    return customer_segments_df\n",
    "\n",
    "# function to ingest streaming data for fraud flags\n",
    "def ingest_fraud_flags():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "\n",
    "    # print to check where df is streaming or not\n",
    "    print(\"transactions df to fraud flags streaming:\", is_streaming(transactions_stream_df))\n",
    "\n",
    "    fraud_flags_stream_df = create_fraud_flags(transactions_stream_df)\n",
    "\n",
    "    # print whether the df is streaming\n",
    "    print(\"Fraud Flags DataFrame is streaming:\", is_streaming(fraud_flags_stream_df))\n",
    "\n",
    "    query = fraud_flags_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.fraud_flag\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .toTable(\"silver.fraud_flags\")\n",
    "  \n",
    "\n",
    "    return query\n",
    "\n",
    "# function to ingest streaming data for customer segments\n",
    "def ingest_customer_segments():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "\n",
    "    customers_stream_df = spark.readStream.table(\"silver.customer\")\n",
    "\n",
    "    # print whether the df's are streaming\n",
    "    print(\"transactions df for customer segments streaming:\", is_streaming(transactions_stream_df))\n",
    "    print(\"customers df is streaming:\", is_streaming(customers_stream_df))\n",
    "\n",
    "    customer_segments_stream_df = create_customer_segments(transactions_stream_df, customers_stream_df)\n",
    "\n",
    "    # print whether the df is streaming\n",
    "    print(\"Customer Segments DataFrame is streaming:\", is_streaming(customer_segments_stream_df))\n",
    "\n",
    "    # for streaming aggregations, using 'Complete' mode\n",
    "    query = customer_segments_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.customer_segments\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .toTable(\"silver.customer_segments\")\n",
    "\n",
    "    return query\n",
    "\n",
    "# starting streaming \n",
    "query_fraud_flags = ingest_fraud_flags()\n",
    "query_customer_segments = ingest_customer_segments()\n",
    "\n",
    "# await termination to streaming queries\n",
    "query_fraud_flags.awaitTermination()\n",
    "query_customer_segments.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "071ee9a9-4027-4ebd-88ea-ea9034aeea50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##New code without trigger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab22ea63-e3fe-4bff-a058-030c174ed12a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions df to fraud flags streaming: True\nFraud Flags DataFrame is streaming: True\ntransactions df for customer segments streaming: True\ncustomers df is streaming: False\nCustomer Segments DataFrame is streaming: True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_timestamp, to_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Real-time Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Function for checking if DataFrame is streaming\n",
    "def is_streaming(df):\n",
    "    return hasattr(df, 'isStreaming') and df.isStreaming\n",
    "\n",
    "# Defining UDF to generate flag_id example 'F001'\n",
    "@udf(StringType())\n",
    "def generate_flag_id(transaction_id):\n",
    "    prefix = \"F\"\n",
    "    suffix = str(transaction_id).zfill(3)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "# Defining UDF to generate segment_id example 'S001'\n",
    "@udf(StringType())\n",
    "def generate_segment_id(customer_id):\n",
    "    prefix = \"S\"\n",
    "    suffix = str(customer_id).zfill(3)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "# Defining transformation logic to fraud_flags table\n",
    "def create_fraud_flags(transactions_df):\n",
    "    fraud_flags_df = transactions_df \\\n",
    "        .withColumn(\"flag_type\", when(col(\"amount\") > 50000, \"unusual_amount\")\n",
    "                    .when(col(\"channel\") == \"mobile\", \"velocity_check\")\n",
    "                    .when(col(\"transaction_type\") == \"transfer\", \"pattern_anomaly\")\n",
    "                    .otherwise(lit(None))) \\\n",
    "        .withColumn(\"confidence_score\", when(col(\"flag_type\") == \"unusual_amount\", 0.9)\n",
    "                    .when(col(\"flag_type\") == \"velocity_check\", 0.8)\n",
    "                    .when(col(\"flag_type\") == \"pattern_anomaly\", 0.7)\n",
    "                    .otherwise(lit(0.5))) \\\n",
    "        .withColumn(\"flag_id\", generate_flag_id(col(\"transaction_id\")))\n",
    "\n",
    "    fraud_flags_df = fraud_flags_df \\\n",
    "        .filter(col(\"flag_type\").isNotNull()) \\\n",
    "        .select(\"flag_id\", \"transaction_id\", \"flag_type\", \"confidence_score\", \"timestamp\")\n",
    "\n",
    "    return fraud_flags_df\n",
    "\n",
    "# Defining transformation logic to customer_segments table\n",
    "def create_customer_segments(transactions_df, customers_df):\n",
    "    # Converting timestamp to TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # Adding watermark to handle late arrival data\n",
    "    transactions_df = transactions_df.withWatermark(\"timestamp\", \"1 day\")\n",
    "\n",
    "    # Joining transactions and customers df\n",
    "    joined_df = transactions_df.join(customers_df, \"customer_id\")\n",
    "\n",
    "    # Creating temporary view for joined data\n",
    "    joined_df.createOrReplaceTempView(\"joined_view\")\n",
    "\n",
    "    # SQL queries for streaming aggregation\n",
    "    high_value_query = \"\"\"\n",
    "        SELECT customer_id, 'High Value' AS segment_name, 'Customers with high transaction volume' AS segment_description\n",
    "        FROM (\n",
    "            SELECT customer_id, SUM(amount) AS total_amount\n",
    "            FROM joined_view\n",
    "            GROUP BY customer_id\n",
    "            HAVING SUM(amount) > 100000\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    new_user_query = \"\"\"\n",
    "        SELECT customer_id, 'New User' AS segment_name, 'Customers who joined in last 30 days' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE join_date > current_date() - INTERVAL 30 DAYS\n",
    "    \"\"\"\n",
    "\n",
    "    inactive_query = \"\"\"\n",
    "        SELECT customer_id, 'Inactive' AS segment_name, 'No transactions in last 90 days' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE last_update < current_date() - INTERVAL 90 DAYS\n",
    "    \"\"\"\n",
    "\n",
    "    credit_risk_query = \"\"\"\n",
    "        SELECT customer_id, 'Credit Risk' AS segment_name, 'Customers with low credit scores' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE credit_score < 600\n",
    "    \"\"\"\n",
    "\n",
    "    loyal_query = \"\"\"\n",
    "        SELECT customer_id, 'Loyal' AS segment_name, 'Consistent activity for over 5 years' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE DATEDIFF(current_date(), join_date) > 5 * 365\n",
    "    \"\"\"\n",
    "\n",
    "    high_value_df = spark.sql(high_value_query)\n",
    "    new_user_df = spark.sql(new_user_query)\n",
    "    inactive_df = spark.sql(inactive_query)\n",
    "    credit_risk_df = spark.sql(credit_risk_query)\n",
    "    loyal_df = spark.sql(loyal_query)\n",
    "\n",
    "    # Combining segments\n",
    "    customer_segments_df = high_value_df.union(new_user_df) \\\n",
    "        .union(inactive_df) \\\n",
    "        .union(credit_risk_df) \\\n",
    "        .union(loyal_df) \\\n",
    "        .withColumn(\"segment_id\", generate_segment_id(col(\"customer_id\"))) \\\n",
    "        .withColumn(\"last_updated\", current_timestamp())\n",
    "\n",
    "    return customer_segments_df\n",
    "\n",
    "# Function to ingest streaming data for fraud flags\n",
    "def ingest_fraud_flags():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "\n",
    "    # Print to check if df is streaming\n",
    "    print(\"transactions df to fraud flags streaming:\", is_streaming(transactions_stream_df))\n",
    "\n",
    "    fraud_flags_stream_df = create_fraud_flags(transactions_stream_df)\n",
    "\n",
    "    # Print whether the df is streaming\n",
    "    print(\"Fraud Flags DataFrame is streaming:\", is_streaming(fraud_flags_stream_df))\n",
    "\n",
    "    query = fraud_flags_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.fraud_flag\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"10 seconds\")  # Set the trigger interval here\n",
    "        .toTable(\"silver.fraud_flags\")\n",
    "  \n",
    "    return query\n",
    "\n",
    "# Function to ingest streaming data for customer segments\n",
    "def ingest_customer_segments():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "    customers_df = spark.read.table(\"silver.customer\")  # Static table\n",
    "\n",
    "    # Print whether the df's are streaming\n",
    "    print(\"transactions df for customer segments streaming:\", is_streaming(transactions_stream_df))\n",
    "    print(\"customers df is streaming:\", is_streaming(customers_df))\n",
    "\n",
    "    customer_segments_stream_df = create_customer_segments(transactions_stream_df, customers_df)\n",
    "\n",
    "    # Print whether the df is streaming\n",
    "    print(\"Customer Segments DataFrame is streaming:\", is_streaming(customer_segments_stream_df))\n",
    "\n",
    "    # For streaming aggregations, using 'Complete' mode\n",
    "    query = customer_segments_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.customer_segments\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .trigger(processingTime=\"10 seconds\")  # Set the trigger interval here\n",
    "        .toTable(\"silver.customer_segments\")\n",
    "\n",
    "    return query\n",
    "\n",
    "# Starting streaming \n",
    "query_fraud_flags = ingest_fraud_flags()\n",
    "query_customer_segments = ingest_customer_segments()\n",
    "\n",
    "# Await termination of streaming queries\n",
    "query_fraud_flags.awaitTermination()\n",
    "query_customer_segments.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a74667a1-4ccf-4ac5-942b-89aec039f3d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##New working code with trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadfaad3-e26e-4ae9-9724-7c0e2d9555f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transactions df to fraud flags streaming: True\nFraud Flags DataFrame is streaming: True\ntransactions df for customer segments streaming: True\ncustomers df is streaming: False\nCustomer Segments DataFrame is streaming: True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_timestamp, to_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "# Function for checking if DataFrame is streaming\n",
    "def is_streaming(df):\n",
    "    return hasattr(df, 'isStreaming') and df.isStreaming\n",
    "\n",
    "# Defining UDF to generate flag_id example 'F001'\n",
    "@udf(StringType())\n",
    "def generate_flag_id(transaction_id):\n",
    "    prefix = \"F\"\n",
    "    suffix = str(transaction_id).zfill(3)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "# Defining UDF to generate segment_id example 'S001'\n",
    "@udf(StringType())\n",
    "def generate_segment_id(customer_id):\n",
    "    prefix = \"S\"\n",
    "    suffix = str(customer_id).zfill(3)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "# Defining transformation logic to fraud_flags table\n",
    "def create_fraud_flags(transactions_df):\n",
    "    fraud_flags_df = transactions_df \\\n",
    "        .withColumn(\"flag_type\", when(col(\"amount\") > 50000, \"unusual_amount\")\n",
    "                    .when(col(\"channel\") == \"mobile\", \"velocity_check\")\n",
    "                    .when(col(\"transaction_type\") == \"transfer\", \"pattern_anomaly\")\n",
    "                    .otherwise(lit(None))) \\\n",
    "        .withColumn(\"confidence_score\", when(col(\"flag_type\") == \"unusual_amount\", 0.9)\n",
    "                    .when(col(\"flag_type\") == \"velocity_check\", 0.8)\n",
    "                    .when(col(\"flag_type\") == \"pattern_anomaly\", 0.7)\n",
    "                    .otherwise(lit(0.5))) \\\n",
    "        .withColumn(\"flag_id\", generate_flag_id(col(\"transaction_id\")))\n",
    "\n",
    "    fraud_flags_df = fraud_flags_df \\\n",
    "        .filter(col(\"flag_type\").isNotNull()) \\\n",
    "        .select(\"flag_id\", \"transaction_id\", \"flag_type\", \"confidence_score\", \"timestamp\")\n",
    "\n",
    "    return fraud_flags_df\n",
    "\n",
    "# Defining transformation logic to customer_segments table\n",
    "def create_customer_segments(transactions_df, customers_df):\n",
    "    # Converting timestamp to TimestampType\n",
    "    transactions_df = transactions_df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # Adding watermark to handle late arrival data\n",
    "    transactions_df = transactions_df.withWatermark(\"timestamp\", \"1 day\")\n",
    "\n",
    "    # Joining transactions and customers df\n",
    "    joined_df = transactions_df.join(customers_df, \"customer_id\")\n",
    "\n",
    "    # Creating temporary view for joined data\n",
    "    joined_df.createOrReplaceTempView(\"joined_view\")\n",
    "\n",
    "    # SQL queries for streaming aggregation\n",
    "    high_value_query = \"\"\"\n",
    "        SELECT customer_id, 'High Value' AS segment_name, 'Customers with high transaction volume' AS segment_description\n",
    "        FROM (\n",
    "            SELECT customer_id, SUM(amount) AS total_amount\n",
    "            FROM joined_view\n",
    "            GROUP BY customer_id\n",
    "            HAVING SUM(amount) > 100000\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    new_user_query = \"\"\"\n",
    "        SELECT customer_id, 'New User' AS segment_name, 'Customers who joined in last 30 days' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE join_date > current_date() - INTERVAL 30 DAYS\n",
    "    \"\"\"\n",
    "\n",
    "    inactive_query = \"\"\"\n",
    "        SELECT customer_id, 'Inactive' AS segment_name, 'No transactions in last 90 days' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE last_update < current_date() - INTERVAL 90 DAYS\n",
    "    \"\"\"\n",
    "\n",
    "    credit_risk_query = \"\"\"\n",
    "        SELECT customer_id, 'Credit Risk' AS segment_name, 'Customers with low credit scores' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE credit_score < 600\n",
    "    \"\"\"\n",
    "\n",
    "    loyal_query = \"\"\"\n",
    "        SELECT customer_id, 'Loyal' AS segment_name, 'Consistent activity for over 5 years' AS segment_description\n",
    "        FROM joined_view\n",
    "        WHERE DATEDIFF(current_date(), join_date) > 5 * 365\n",
    "    \"\"\"\n",
    "\n",
    "    high_value_df = spark.sql(high_value_query)\n",
    "    new_user_df = spark.sql(new_user_query)\n",
    "    inactive_df = spark.sql(inactive_query)\n",
    "    credit_risk_df = spark.sql(credit_risk_query)\n",
    "    loyal_df = spark.sql(loyal_query)\n",
    "\n",
    "    # Combining segments\n",
    "    customer_segments_df = high_value_df.union(new_user_df) \\\n",
    "        .union(inactive_df) \\\n",
    "        .union(credit_risk_df) \\\n",
    "        .union(loyal_df) \\\n",
    "        .withColumn(\"segment_id\", generate_segment_id(col(\"customer_id\"))) \\\n",
    "        .withColumn(\"last_updated\", current_timestamp())\n",
    "\n",
    "    return customer_segments_df\n",
    "\n",
    "# Function to ingest streaming data for fraud flags\n",
    "def ingest_fraud_flags():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "\n",
    "    # Print to check if df is streaming\n",
    "    print(\"transactions df to fraud flags streaming:\", is_streaming(transactions_stream_df))\n",
    "\n",
    "    fraud_flags_stream_df = create_fraud_flags(transactions_stream_df)\n",
    "\n",
    "    # Print whether the df is streaming\n",
    "    print(\"Fraud Flags DataFrame is streaming:\", is_streaming(fraud_flags_stream_df))\n",
    "\n",
    "    query = fraud_flags_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.fraud_flag\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .toTable(\"silver.fraud_flags\")\n",
    "  \n",
    "    return query\n",
    "\n",
    "# Function to ingest streaming data for customer segments\n",
    "def ingest_customer_segments():\n",
    "    transactions_stream_df = spark.readStream.table(\"silver.transactions\")\n",
    "    customers_df = spark.read.table(\"silver.customer\")  # Static table\n",
    "\n",
    "    # Print whether the df's are streaming\n",
    "    print(\"transactions df for customer segments streaming:\", is_streaming(transactions_stream_df))\n",
    "    print(\"customers df is streaming:\", is_streaming(customers_df))\n",
    "\n",
    "    customer_segments_stream_df = create_customer_segments(transactions_stream_df, customers_df)\n",
    "\n",
    "    # Print whether the df is streaming\n",
    "    print(\"Customer Segments DataFrame is streaming:\", is_streaming(customer_segments_stream_df))\n",
    "\n",
    "    # For streaming aggregations, using 'Complete' mode\n",
    "    query = customer_segments_stream_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", \"silver.checkpoint.customer_segments\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .toTable(\"silver.customer_segments\")\n",
    "\n",
    "    return query\n",
    "\n",
    "# Starting streaming \n",
    "query_fraud_flags = ingest_fraud_flags()\n",
    "query_customer_segments = ingest_customer_segments()\n",
    "\n",
    "# Await termination of streaming queries\n",
    "query_fraud_flags.awaitTermination()\n",
    "query_customer_segments.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraudflags & customer segement streaming & load to silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
