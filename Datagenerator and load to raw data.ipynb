{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc92b33-da84-4511-a73a-1578ba627c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## This code will generate data and store in DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d86db5-e238-4838-8794-73a844824e72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.io.IOException: Connection failed\n",
       "\tat com.databricks.rpc.Jetty9Client$$anon$1.handleError(Jetty9Client.scala:848)\n",
       "\tat com.databricks.rpc.Jetty9Client$$anon$1.onFailure(Jetty9Client.scala:775)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:197)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:189)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:275)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:247)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:164)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:821)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:559)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.failed(HttpDestination.java:313)\n",
       "\tat com.databricks.rpc.AbstractConnectionPool$1.failed(AbstractConnectionPool.java:161)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n",
       "\tat com.databricks.rpc.Jetty9Client$DatabricksHttpDestinationOverHTTP$$anon$2.failed(Jetty9Client.scala:1671)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:660)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport.connectFailed(AbstractConnectorHttpClientTransport.java:138)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractConnectorHttpClientTransport.java:188)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:966)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:369)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:173)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:279)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:275)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:125)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:173)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:125)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:167)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "Caused by: java.net.ConnectException: Connection refused\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n",
       "\t... 26 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.io.IOException: Connection failed\n\tat com.databricks.rpc.Jetty9Client$$anon$1.handleError(Jetty9Client.scala:848)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onFailure(Jetty9Client.scala:775)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:197)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyFailure(ResponseNotifier.java:189)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.notifyFailureComplete(HttpExchange.java:275)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpExchange.abort(HttpExchange.java:247)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpConversation.abort(HttpConversation.java:164)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpRequest.abort(HttpRequest.java:821)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.abort(HttpDestination.java:559)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpDestination.failed(HttpDestination.java:313)\n\tat com.databricks.rpc.AbstractConnectionPool$1.failed(AbstractConnectionPool.java:161)\n\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n\tat com.databricks.rpc.Jetty9Client$DatabricksHttpDestinationOverHTTP$$anon$2.failed(Jetty9Client.scala:1671)\n\tat shaded.v9_4.org.eclipse.jetty.util.Promise$Wrapper.failed(Promise.java:136)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpClient$1$1.failed(HttpClient.java:660)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport.connectFailed(AbstractConnectorHttpClientTransport.java:138)\n\tat shaded.v9_4.org.eclipse.jetty.client.AbstractConnectorHttpClientTransport$ClientSelectorManager.connectionFailed(AbstractConnectorHttpClientTransport.java:188)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$Connect.failed(ManagedSelector.java:966)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:369)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:173)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:279)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:275)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:125)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:173)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:125)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:167)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n\t... 26 more\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install faker\n",
    "from faker import Faker\n",
    "from random import randint, choice\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize the Faker generator\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema for each table\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"join_date\", TimestampType(), True),\n",
    "    StructField(\"last_update\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "branches_schema = StructType([\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"timezone\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the tables and their schemas\n",
    "tables = {\n",
    "    \"transactions\": transactions_schema,\n",
    "    \"customers\": customers_schema,\n",
    "    \"branches\": branches_schema,\n",
    "}\n",
    "\n",
    "# Function to generate data for a specific table\n",
    "def generate_data(table_name, num_entries=1):\n",
    "    data = []\n",
    "    for _ in range(num_entries):\n",
    "        row = {}\n",
    "        for column in tables[table_name].fieldNames():\n",
    "            if column == 'transaction_id':\n",
    "                row[column] = \"T\" + str(randint(1000, 9999))\n",
    "            elif column == 'customer_id':\n",
    "                row[column] = \"C\" + str(randint(3000, 8000))\n",
    "            elif column == 'branch_id':\n",
    "                row[column] = \"B\" + str(randint(101, 110)).zfill(3)\n",
    "            elif column == 'channel':\n",
    "                row[column] = choice(['online', 'branch', 'mobile', 'ATM'])\n",
    "            elif column == 'transaction_type':\n",
    "                row[column] = choice(['deposit', 'withdrawal', 'transfer', 'balance_check'])\n",
    "            elif column == 'amount':\n",
    "                row[column] = round(randint(1, 1000) * 100, 2)\n",
    "            elif column == 'currency':\n",
    "                row[column] = choice(['USD', 'EUR', 'GBP'])\n",
    "            elif column == 'timestamp':\n",
    "                row[column] = fake.date_time_this_year()\n",
    "            elif column == 'status':\n",
    "                row[column] = choice(['success', 'pending', 'failed'])\n",
    "            elif column == 'name':\n",
    "                if table_name == \"branches\":\n",
    "                    row[column] = fake.company_suffix() + \" Branch\"\n",
    "                else:\n",
    "                    row[column] = fake.name()\n",
    "            elif column == 'email':\n",
    "                first_name = fake.first_name().lower()\n",
    "                row[column] = f\"{first_name}85@example.net\"\n",
    "            elif column == 'phone':\n",
    "                row[column] = ''.join([str(randint(0, 9)) for _ in range(10)])\n",
    "            elif column == 'address':\n",
    "                row[column] = fake.address()\n",
    "            elif column == 'credit_score':\n",
    "                row[column] = randint(300, 850)\n",
    "            elif column == 'join_date':\n",
    "                row[column] = fake.date_time_between(start_date='-5y', end_date=datetime.today())\n",
    "            elif column == 'last_update':\n",
    "                row[column] = fake.date_time_between(start_date='-1y', end_date=datetime.today())\n",
    "            elif column == 'location':\n",
    "                if table_name == \"branches\":\n",
    "                    row[column] = choice(['New York, USA', 'London, UK', 'Toronto, Canada', 'Sydney, Australia', 'Los Angeles, USA'])\n",
    "                else:\n",
    "                    row[column] = fake.city()\n",
    "            elif column == 'timezone':\n",
    "                if table_name == \"branches\":\n",
    "                    row[column] = choice(['EST', 'GMT', 'EST', 'AEST', 'PST'])\n",
    "                else:\n",
    "                    row[column] = choice(['UTC', 'EST', 'CST', 'MST', 'PST'])\n",
    "            else:\n",
    "                row[column] = fake.uuid4()\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate static data for customers and branches\n",
    "customers_pd_df = generate_data(\"customers\", num_entries=1000)\n",
    "branches_pd_df = generate_data(\"branches\", num_entries=10)\n",
    "\n",
    "# Convert to Spark DataFrames with correct schema\n",
    "customers_spark_df = spark.createDataFrame(customers_pd_df, schema=customers_schema)\n",
    "branches_spark_df = spark.createDataFrame(branches_pd_df, schema=branches_schema)\n",
    "\n",
    "\n",
    "\n",
    "# Write the static data to DBFS\n",
    "customers_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"raw_data.customers\")\n",
    "branches_spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"raw_data.branches\")\n",
    "\n",
    "# Function to stream transactions\n",
    "def stream_data():\n",
    "    while True:\n",
    "        # Generate transactions data\n",
    "        transactions_pd_df = generate_data(\"transactions\", num_entries=5)\n",
    "        transactions_pd_df['timestamp'] = pd.to_datetime(transactions_pd_df['timestamp'])\n",
    "        transactions_spark_df = spark.createDataFrame(transactions_pd_df, schema=transactions_schema)\n",
    "        transactions_spark_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"raw_data.transactions\")\n",
    "        print(\"Generated and saved transactions data\")\n",
    "        sleep(10)  # Adjust the sleep duration as needed\n",
    "\n",
    "# Start the data streaming\n",
    "stream_data()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Datagenerator and load to raw data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
