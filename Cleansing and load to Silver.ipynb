{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8a10c3-5e8f-4310-9b87-391705d354c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c917ab9-3f85-4167-92b1-f204cdd55555",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cleansing rules to be followed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c0e9c1-4785-483f-8ed4-32acb9288edc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Drops rows with any null values.\n",
    "##### Ensures columns are of the correct data type based on the schema.\n",
    "##### Removes duplicate rows.\n",
    "##### Trims whitespace and removes special characters from text columns.\n",
    "##### Handles outliers using the IQR method.\n",
    "##### Ensures data consistency and checks for non-null and non-NaN values.</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834ceda6-c4ef-487e-b683-996b82e9a4e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h4> Here we are creating Dynamic function for cleansing the All the customers,Branch, Transaction Data </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2da3b6b2-6883-4481-a5c9-0df9b6680d37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef96f7cf-d9d0-4ff2-a20b-f41f67da7262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming transactions started...\nbatch customers completed...\nbatch branches completed...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    trim,\n",
    "    regexp_replace,\n",
    "    to_timestamp,\n",
    "    isnan,\n",
    "    date_format,\n",
    ")\n",
    "\n",
    "\n",
    "# transactions schema\n",
    "transaction_schema = StructType(\n",
    "    [\n",
    "        StructField(\"transaction_id\", StringType(), False),\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"branch_id\", StringType(), False),\n",
    "        StructField(\"channel\", StringType(), False),\n",
    "        StructField(\"transaction_type\", StringType(), False),\n",
    "        StructField(\"amount\", DoubleType(), False),\n",
    "        StructField(\"currency\", StringType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# customer schema\n",
    "customer_schema = StructType(\n",
    "    [\n",
    "        StructField(\"customer_id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True),\n",
    "        StructField(\"address\", StringType(), True),\n",
    "        StructField(\"credit_score\", LongType(), True),\n",
    "        StructField(\"join_date\", TimestampType(), True),\n",
    "        StructField(\"last_update\", TimestampType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# branch schema\n",
    "branch_schema = StructType(\n",
    "    [\n",
    "        StructField(\"branch_id\", StringType(), False),\n",
    "        StructField(\"name\", StringType(), False),  # Changed from branch_name to name\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"timezone\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# cleanse functions\n",
    "def handle_missing_values(df):\n",
    "    return df.na.drop()\n",
    "\n",
    "\n",
    "def convert_data_types(df, schema):\n",
    "    for field in schema:\n",
    "        if field.dataType == LongType():\n",
    "            df = df.withColumn(field.name, col(field.name).cast(LongType()))\n",
    "        elif field.dataType == StringType():\n",
    "            df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "        elif field.dataType == TimestampType():\n",
    "            df = df.withColumn(field.name, to_timestamp(col(field.name)))\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_timestamps(df, timestamp_columns):\n",
    "    for column, fmt in timestamp_columns.items():\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, date_format(col(column), fmt))\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "\n",
    "def clean_text_data(df, columns, exclude_columns=[]):\n",
    "    for column in columns:\n",
    "        if column in df.columns and column not in exclude_columns:\n",
    "            df = df.withColumn(column, trim(col(column)))\n",
    "            df = df.withColumn(\n",
    "                column, regexp_replace(col(column), \"[^a-zA-Z0-9\\s]\", \"\")\n",
    "            )\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_outliers(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            # using static threshold\n",
    "            threshold = 1000000\n",
    "            df = df.filter((col(column) >= -threshold) & (col(column) <= threshold))\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_quality_checks(df, columns):\n",
    "    for column in columns:\n",
    "        if column in df.columns:\n",
    "            df = df.filter(col(column).isNotNull())\n",
    "            df = df.filter(~isnan(col(column)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Applying to transactions\n",
    "def cleanse_transaction(df):\n",
    "    df = handle_missing_values(df)\n",
    "    df = convert_data_types(df, transaction_schema)\n",
    "    df = format_timestamps(df, {\"timestamp\": \"yyyy-MM-dd HH:mm:ss\"})\n",
    "    df = remove_duplicates(df)\n",
    "    df = clean_text_data(df, [\"channel\", \"transaction_type\", \"currency\", \"status\"])\n",
    "    df = handle_outliers(df, [\"amount\"])\n",
    "    df = data_quality_checks(df, [\"transaction_id\", \"amount\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Applying to customers\n",
    "def cleanse_customer(df):\n",
    "    df = handle_missing_values(df)\n",
    "    df = convert_data_types(df, customer_schema)\n",
    "    df = format_timestamps(\n",
    "        df, {\"join_date\": \"yyyy-MM-dd\", \"last_update\": \"yyyy-MM-dd HH:mm:ss\"}\n",
    "    )\n",
    "    df = remove_duplicates(df)\n",
    "    df = clean_text_data(\n",
    "        df, [\"name\", \"email\", \"phone\", \"address\"], exclude_columns=[\"email\"]\n",
    "    )\n",
    "    df = data_quality_checks(df, [\"customer_id\", \"credit_score\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Applying to branch\n",
    "def cleanse_branch(df):\n",
    "    df = handle_missing_values(df)\n",
    "    df = convert_data_types(df, branch_schema)\n",
    "    df = remove_duplicates(df)\n",
    "    # Changed branch_name to name\n",
    "    df = clean_text_data(df, [\"name\", \"location\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# function for cleaning all data\n",
    "def process_and_cleanse_data():\n",
    "    # transaction streaming\n",
    "    transactions_stream_df = spark.readStream.table(\"Bronze_layer.transactions\")\n",
    "\n",
    "    cleaned_transaction_stream_df = cleanse_transaction(transactions_stream_df)\n",
    "\n",
    "    # writing stream\n",
    "    transaction_query = (\n",
    "        cleaned_transaction_stream_df.writeStream.format(\"delta\")\n",
    "        .option(\"checkpointLocation\", \"Silver_layer.checkpoint\")\n",
    "        .outputMode(\"append\")\n",
    "        .toTable(\"Silver_layer.transactions\")\n",
    "    )\n",
    "\n",
    "    print(\"Streaming transactions started...\")\n",
    "\n",
    "    # Processing customer\n",
    "    customer_df = spark.read.table(\"Bronze_layer.customers\")\n",
    "\n",
    "    cleaned_customer_df = cleanse_customer(customer_df)\n",
    "\n",
    "    cleaned_customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "        \"Silver_layer.customers\"\n",
    "    )\n",
    "\n",
    "    print(\"batch customers completed...\")\n",
    "\n",
    "    # processing branch\n",
    "    branch_df = spark.read.table(\"Bronze_layer.branches\")\n",
    "\n",
    "    cleaned_branch_df = cleanse_branch(branch_df)\n",
    "\n",
    "    cleaned_branch_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "        \"Silver_layer.branches\"\n",
    "    )\n",
    "\n",
    "    print(\"batch branches completed...\")\n",
    "\n",
    "    return transaction_query\n",
    "\n",
    "\n",
    "# starting processing and cleansing all data\n",
    "transaction_streaming = process_and_cleanse_data()\n",
    "\n",
    "if transaction_streaming:\n",
    "    try:\n",
    "        transaction_streaming.awaitTermination()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Streaming interrupted.\")\n",
    "else:\n",
    "    print(\"Streaming not started.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cleansing and load to Silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
